
#pragma once
#ifndef TEST_CASE_H
#define TEST_CASE_H

#ifdef __cplusplus
extern "C" {
#endif


#include <stdint.h>
#include <stdbool.h>

// Compiler feature macros adapted from Hedley (public domain)
// https://github.com/nemequ/hedley

#if defined(__has_builtin)
#  define EXO_HAS_BUILTIN(builtin) __has_builtin(builtin)
#else
#  define EXO_HAS_BUILTIN(builtin) (0)
#endif

#if EXO_HAS_BUILTIN(__builtin_assume)
#  define EXO_ASSUME(expr) __builtin_assume(expr)
#elif EXO_HAS_BUILTIN(__builtin_unreachable)
#  define EXO_ASSUME(expr) \
      ((void)((expr) ? 1 : (__builtin_unreachable(), 1)))
#else
#  define EXO_ASSUME(expr) ((void)(expr))
#endif


#include <stdio.h>
#include <stdlib.h>

#ifndef EXO_WIN_1UI16
#define EXO_WIN_1UI16
struct exo_win_1ui16{
    uint16_t * const data;
    const int_fast32_t strides[1];
};
#endif
#ifndef EXO_WIN_1UI16C
#define EXO_WIN_1UI16C
struct exo_win_1ui16c{
    const uint16_t * const data;
    const int_fast32_t strides[1];
};
#endif
// exo_base_blur(
//     W : size,
//     H : size,
//     blur_y : ui16[H, W] @DRAM,
//     inp : ui16[H + 2, W + 2] @DRAM
// )
void exo_base_blur( void *ctxt, int_fast32_t W, int_fast32_t H, uint16_t* blur_y, const uint16_t* inp );

// exo_blur_halide(
//     W : size,
//     H : size,
//     blur_y : ui16[H, W] @DRAM,
//     inp : ui16[H + 2, W + 2] @DRAM
// )
void exo_blur_halide( void *ctxt, int_fast32_t W, int_fast32_t H, uint16_t* blur_y, const uint16_t* inp );



#ifdef __cplusplus
}
#endif
#endif  // TEST_CASE_H

#include "test_case.h"

#include <immintrin.h>
#include <stdio.h>
#include <stdlib.h>

#ifndef EXO_WIN_1UI16_AVX2
#define EXO_WIN_1UI16_AVX2
struct exo_win_1ui16_AVX2{
    __m256i * const data;
    const int_fast32_t strides[1];
};
#endif
#ifndef EXO_WIN_1UI16C_AVX2
#define EXO_WIN_1UI16C_AVX2
struct exo_win_1ui16c_AVX2{
    const __m256i * const data;
    const int_fast32_t strides[1];
};
#endif

/* relying on the following instruction..."
avx2_ui16_divide_by_3(out,x)

    {{
        {out_data} = _mm256_mulhi_epu16({x_data}, _mm256_set1_epi16(43691));
        {out_data} = _mm256_srli_epi16({out_data}, 1);
    }}
    
*/
// exo_base_blur(
//     W : size,
//     H : size,
//     blur_y : ui16[H, W] @DRAM,
//     inp : ui16[H + 2, W + 2] @DRAM
// )
void exo_base_blur( void *ctxt, int_fast32_t W, int_fast32_t H, uint16_t* blur_y, const uint16_t* inp ) {
EXO_ASSUME(H % 32 == 0);
EXO_ASSUME(W % 256 == 0);
uint16_t *blur_x = (uint16_t*) malloc((H + 2) * W * sizeof(*blur_x));
for (int_fast32_t y = 0; y < H + 2; y++) {
  for (int_fast32_t x = 0; x < W; x++) {
    blur_x[y * W + x] = (inp[y * (W + 2) + x] + inp[y * (W + 2) + x + 1] + inp[y * (W + 2) + x + 2]) / ((uint16_t) 3.0);
  }
}
for (int_fast32_t y = 0; y < H; y++) {
  for (int_fast32_t x = 0; x < W; x++) {
    blur_y[y * W + x] = (blur_x[y * W + x] + blur_x[(y + 1) * W + x] + blur_x[(y + 2) * W + x]) / ((uint16_t) 3.0);
  }
}
free(blur_x);
}

// exo_blur_halide(
//     W : size,
//     H : size,
//     blur_y : ui16[H, W] @DRAM,
//     inp : ui16[H + 2, W + 2] @DRAM
// )
void exo_blur_halide( void *ctxt, int_fast32_t W, int_fast32_t H, uint16_t* blur_y, const uint16_t* inp ) {
EXO_ASSUME(H % 32 == 0);
EXO_ASSUME(W % 256 == 0);
#pragma omp parallel for
for (int_fast32_t y = 0; y < ((H) / (32)); y++) {
  for (int_fast32_t x = 0; x < ((W) / (256)); x++) {
    uint16_t blur_x[34 * 256];
    for (int_fast32_t yi = 0; yi < 34; yi++) {
      for (int_fast32_t xio = 0; xio < 16; xio++) {
        __m256i var0;
        __m256i var1;
        __m256i var2;
        __m256i var3;
        __m256i var4;
        __m256i var5;
        var3 = _mm256_loadu_si256((const __m256i *) &inp[(yi + 32 * y) * (W + 2) + 16 * xio + 256 * x]);
        var4 = _mm256_loadu_si256((const __m256i *) &inp[(yi + 32 * y) * (W + 2) + 1 + 16 * xio + 256 * x]);
        var2 = _mm256_adds_epu16(var3, var4);
        var5 = _mm256_loadu_si256((const __m256i *) &inp[(yi + 32 * y) * (W + 2) + 2 + 16 * xio + 256 * x]);
        var1 = _mm256_adds_epu16(var2, var5);
        
    {
        var0 = _mm256_mulhi_epu16(var1, _mm256_set1_epi16(43691));
        var0 = _mm256_srli_epi16(var0, 1);
    }
    
        _mm256_storeu_si256((__m256i *) &blur_x[(yi) * (256) + 16 * xio], var0);
      }
    }
    for (int_fast32_t yi = 0; yi < 32; yi++) {
      for (int_fast32_t xio = 0; xio < 16; xio++) {
        __m256i var6;
        __m256i var7;
        __m256i var8;
        __m256i var9;
        __m256i var10;
        __m256i var11;
        var9 = _mm256_loadu_si256((const __m256i *) &blur_x[(yi) * (256) + 16 * xio]);
        var10 = _mm256_loadu_si256((const __m256i *) &blur_x[(1 + yi) * (256) + 16 * xio]);
        var8 = _mm256_adds_epu16(var9, var10);
        var11 = _mm256_loadu_si256((const __m256i *) &blur_x[(2 + yi) * (256) + 16 * xio]);
        var7 = _mm256_adds_epu16(var8, var11);
        
    {
        var6 = _mm256_mulhi_epu16(var7, _mm256_set1_epi16(43691));
        var6 = _mm256_srli_epi16(var6, 1);
    }
    
        _mm256_storeu_si256((__m256i *) &blur_y[(yi + 32 * y) * W + 16 * xio + 256 * x], var6);
      }
    }
  }
}
}


/* relying on the following instruction..."
mm256_add_epi16(out,x,y)
{out_data} = _mm256_adds_epu16({x_data}, {y_data});
*/

/* relying on the following instruction..."
mm256_loadu_si256(dst,src)
{dst_data} = _mm256_loadu_si256((const __m256i *) &{src_data});
*/

/* relying on the following instruction..."
mm256_storeu_si256(dst,src)
_mm256_storeu_si256((__m256i *) &{dst_data}, {src_data});
*/
