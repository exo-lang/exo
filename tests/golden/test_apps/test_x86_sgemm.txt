
#pragma once
#ifndef TEST_CASE_H
#define TEST_CASE_H

#ifdef __cplusplus
extern "C" {
#endif


#include <stdint.h>
#include <stdbool.h>

// Compiler feature macros adapted from Hedley (public domain)
// https://github.com/nemequ/hedley

#if defined(__has_builtin)
#  define EXO_HAS_BUILTIN(builtin) __has_builtin(builtin)
#else
#  define EXO_HAS_BUILTIN(builtin) (0)
#endif

#if EXO_HAS_BUILTIN(__builtin_assume)
#  define EXO_ASSUME(expr) __builtin_assume(expr)
#elif EXO_HAS_BUILTIN(__builtin_unreachable)
#  define EXO_ASSUME(expr) \
      ((void)((expr) ? 1 : (__builtin_unreachable(), 1)))
#else
#  define EXO_ASSUME(expr) ((void)(expr))
#endif


#include <stdio.h>
#include <stdlib.h>

#ifndef EXO_WIN_1F32
#define EXO_WIN_1F32
struct exo_win_1f32{
    float * const data;
    const int_fast32_t strides[1];
};
#endif
#ifndef EXO_WIN_1F32C
#define EXO_WIN_1F32C
struct exo_win_1f32c{
    const float * const data;
    const int_fast32_t strides[1];
};
#endif
#ifndef EXO_WIN_2F32
#define EXO_WIN_2F32
struct exo_win_2f32{
    float * const data;
    const int_fast32_t strides[2];
};
#endif
#ifndef EXO_WIN_2F32C
#define EXO_WIN_2F32C
struct exo_win_2f32c{
    const float * const data;
    const int_fast32_t strides[2];
};
#endif
// sgemm_exo(
//     M : size,
//     N : size,
//     K : size,
//     A : f32[M, K] @DRAM,
//     B : f32[K, N] @DRAM,
//     C : f32[M, N] @DRAM
// )
void sgemm_exo( void *ctxt, int_fast32_t M, int_fast32_t N, int_fast32_t K, const float* A, const float* B, float* C );



#ifdef __cplusplus
}
#endif
#endif  // TEST_CASE_H

#include "test_case.h"

#include <immintrin.h>
#include <stdio.h>
#include <stdlib.h>

#ifndef EXO_WIN_1F32_AVX512
#define EXO_WIN_1F32_AVX512
struct exo_win_1f32_AVX512{
    __m512 * const data;
    const int_fast32_t strides[1];
};
#endif
#ifndef EXO_WIN_1F32C_AVX512
#define EXO_WIN_1F32C_AVX512
struct exo_win_1f32c_AVX512{
    const __m512 * const data;
    const int_fast32_t strides[1];
};
#endif
// basic_kernel_1x4(
//     K : size,
//     A : [f32][1, K] @DRAM,
//     B : [f32][K, 64] @DRAM,
//     C : [f32][1, 64] @DRAM
// )
static void basic_kernel_1x4( void *ctxt, int_fast32_t K, struct exo_win_2f32c A, struct exo_win_2f32c B, struct exo_win_2f32 C );

// basic_kernel_2x4(
//     K : size,
//     A : [f32][2, K] @DRAM,
//     B : [f32][K, 64] @DRAM,
//     C : [f32][2, 64] @DRAM
// )
static void basic_kernel_2x4( void *ctxt, int_fast32_t K, struct exo_win_2f32c A, struct exo_win_2f32c B, struct exo_win_2f32 C );

// basic_kernel_3x4(
//     K : size,
//     A : [f32][3, K] @DRAM,
//     B : [f32][K, 64] @DRAM,
//     C : [f32][3, 64] @DRAM
// )
static void basic_kernel_3x4( void *ctxt, int_fast32_t K, struct exo_win_2f32c A, struct exo_win_2f32c B, struct exo_win_2f32 C );

// basic_kernel_4x4(
//     K : size,
//     A : [f32][4, K] @DRAM,
//     B : [f32][K, 64] @DRAM,
//     C : [f32][4, 64] @DRAM
// )
static void basic_kernel_4x4( void *ctxt, int_fast32_t K, struct exo_win_2f32c A, struct exo_win_2f32c B, struct exo_win_2f32 C );

// basic_kernel_5x4(
//     K : size,
//     A : [f32][5, K] @DRAM,
//     B : [f32][K, 64] @DRAM,
//     C : [f32][5, 64] @DRAM
// )
static void basic_kernel_5x4( void *ctxt, int_fast32_t K, struct exo_win_2f32c A, struct exo_win_2f32c B, struct exo_win_2f32 C );

// basic_kernel_6x4(
//     K : size,
//     A : [f32][6, K] @DRAM,
//     B : [f32][K, 64] @DRAM,
//     C : [f32][6, 64] @DRAM
// )
static void basic_kernel_6x4( void *ctxt, int_fast32_t K, struct exo_win_2f32c A, struct exo_win_2f32c B, struct exo_win_2f32 C );

// bottom_panel_kernel_scheduled(
//     M : size,
//     K : size,
//     A : [f32][M, K] @DRAM,
//     B : [f32][K, 64] @DRAM,
//     C : [f32][M, 64] @DRAM
// )
static void bottom_panel_kernel_scheduled( void *ctxt, int_fast32_t M, int_fast32_t K, struct exo_win_2f32c A, struct exo_win_2f32c B, struct exo_win_2f32 C );

// right_panel_kernel0(
//     N : size,
//     K : size,
//     A : [f32][6, K] @DRAM,
//     B : [f32][K, N] @DRAM,
//     C : [f32][6, N] @DRAM
// )
static void right_panel_kernel0( void *ctxt, int_fast32_t N, int_fast32_t K, struct exo_win_2f32c A, struct exo_win_2f32c B, struct exo_win_2f32 C );

// right_panel_kernel1(
//     N : size,
//     K : size,
//     A : [f32][6, K] @DRAM,
//     B : [f32][K, N] @DRAM,
//     C : [f32][6, N] @DRAM
// )
static void right_panel_kernel1( void *ctxt, int_fast32_t N, int_fast32_t K, struct exo_win_2f32c A, struct exo_win_2f32c B, struct exo_win_2f32 C );

// right_panel_kernel2(
//     N : size,
//     K : size,
//     A : [f32][6, K] @DRAM,
//     B : [f32][K, N] @DRAM,
//     C : [f32][6, N] @DRAM
// )
static void right_panel_kernel2( void *ctxt, int_fast32_t N, int_fast32_t K, struct exo_win_2f32c A, struct exo_win_2f32c B, struct exo_win_2f32 C );

// right_panel_kernel3(
//     N : size,
//     K : size,
//     A : [f32][6, K] @DRAM,
//     B : [f32][K, N] @DRAM,
//     C : [f32][6, N] @DRAM
// )
static void right_panel_kernel3( void *ctxt, int_fast32_t N, int_fast32_t K, struct exo_win_2f32c A, struct exo_win_2f32c B, struct exo_win_2f32 C );

// right_panel_kernel_scheduled(
//     N : size,
//     K : size,
//     A : [f32][6, K] @DRAM,
//     B : [f32][K, N] @DRAM,
//     C : [f32][6, N] @DRAM
// )
static void right_panel_kernel_scheduled( void *ctxt, int_fast32_t N, int_fast32_t K, struct exo_win_2f32c A, struct exo_win_2f32c B, struct exo_win_2f32 C );

// sgemm_above_kernel(
//     M : size,
//     N : size,
//     K : size,
//     A : [f32][M, K] @DRAM,
//     B : [f32][K, N] @DRAM,
//     C : [f32][M, N] @DRAM
// )
static void sgemm_above_kernel( void *ctxt, int_fast32_t M, int_fast32_t N, int_fast32_t K, struct exo_win_2f32c A, struct exo_win_2f32c B, struct exo_win_2f32 C );

// basic_kernel_1x4(
//     K : size,
//     A : [f32][1, K] @DRAM,
//     B : [f32][K, 64] @DRAM,
//     C : [f32][1, 64] @DRAM
// )
static void basic_kernel_1x4( void *ctxt, int_fast32_t K, struct exo_win_2f32c A, struct exo_win_2f32c B, struct exo_win_2f32 C ) {
EXO_ASSUME(K >= 1);
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
__m512 C_reg[1][4];
for (int_fast32_t i0 = 0; i0 < 1; i0++) {
  C_reg[i0][0] = _mm512_loadu_ps(&C.data[(i0) * (C.strides[0])]);
  C_reg[i0][1] = _mm512_loadu_ps(&C.data[(i0) * (C.strides[0]) + 16]);
  C_reg[i0][2] = _mm512_loadu_ps(&C.data[(i0) * (C.strides[0]) + 32]);
  C_reg[i0][3] = _mm512_loadu_ps(&C.data[(i0) * (C.strides[0]) + 48]);
}
for (int_fast32_t k = 0; k < K; k++) {
  for (int_fast32_t i = 0; i < 1; i++) {
    __m512 var0;
    var0 = _mm512_set1_ps(A.data[(i) * (A.strides[0]) + k]);
    __m512 var1;
    var1 = _mm512_loadu_ps(&B.data[(k) * (B.strides[0])]);
    C_reg[i][0] = _mm512_fmadd_ps(var0, var1, C_reg[i][0]);
    __m512 var1_1;
    var1_1 = _mm512_loadu_ps(&B.data[(k) * (B.strides[0]) + 16]);
    C_reg[i][1] = _mm512_fmadd_ps(var0, var1_1, C_reg[i][1]);
    __m512 var1_2;
    var1_2 = _mm512_loadu_ps(&B.data[(k) * (B.strides[0]) + 32]);
    C_reg[i][2] = _mm512_fmadd_ps(var0, var1_2, C_reg[i][2]);
    __m512 var1_3;
    var1_3 = _mm512_loadu_ps(&B.data[(k) * (B.strides[0]) + 48]);
    C_reg[i][3] = _mm512_fmadd_ps(var0, var1_3, C_reg[i][3]);
  }
}
for (int_fast32_t i0 = 0; i0 < 1; i0++) {
  _mm512_storeu_ps(&C.data[(i0) * (C.strides[0])], C_reg[i0][0]);
  _mm512_storeu_ps(&C.data[(i0) * (C.strides[0]) + 16], C_reg[i0][1]);
  _mm512_storeu_ps(&C.data[(i0) * (C.strides[0]) + 32], C_reg[i0][2]);
  _mm512_storeu_ps(&C.data[(i0) * (C.strides[0]) + 48], C_reg[i0][3]);
}
}

// basic_kernel_2x4(
//     K : size,
//     A : [f32][2, K] @DRAM,
//     B : [f32][K, 64] @DRAM,
//     C : [f32][2, 64] @DRAM
// )
static void basic_kernel_2x4( void *ctxt, int_fast32_t K, struct exo_win_2f32c A, struct exo_win_2f32c B, struct exo_win_2f32 C ) {
EXO_ASSUME(K >= 1);
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
__m512 C_reg[2][4];
for (int_fast32_t i0 = 0; i0 < 2; i0++) {
  C_reg[i0][0] = _mm512_loadu_ps(&C.data[(i0) * (C.strides[0])]);
  C_reg[i0][1] = _mm512_loadu_ps(&C.data[(i0) * (C.strides[0]) + 16]);
  C_reg[i0][2] = _mm512_loadu_ps(&C.data[(i0) * (C.strides[0]) + 32]);
  C_reg[i0][3] = _mm512_loadu_ps(&C.data[(i0) * (C.strides[0]) + 48]);
}
for (int_fast32_t k = 0; k < K; k++) {
  for (int_fast32_t i = 0; i < 2; i++) {
    __m512 var0;
    var0 = _mm512_set1_ps(A.data[(i) * (A.strides[0]) + k]);
    __m512 var1;
    var1 = _mm512_loadu_ps(&B.data[(k) * (B.strides[0])]);
    C_reg[i][0] = _mm512_fmadd_ps(var0, var1, C_reg[i][0]);
    __m512 var1_1;
    var1_1 = _mm512_loadu_ps(&B.data[(k) * (B.strides[0]) + 16]);
    C_reg[i][1] = _mm512_fmadd_ps(var0, var1_1, C_reg[i][1]);
    __m512 var1_2;
    var1_2 = _mm512_loadu_ps(&B.data[(k) * (B.strides[0]) + 32]);
    C_reg[i][2] = _mm512_fmadd_ps(var0, var1_2, C_reg[i][2]);
    __m512 var1_3;
    var1_3 = _mm512_loadu_ps(&B.data[(k) * (B.strides[0]) + 48]);
    C_reg[i][3] = _mm512_fmadd_ps(var0, var1_3, C_reg[i][3]);
  }
}
for (int_fast32_t i0 = 0; i0 < 2; i0++) {
  _mm512_storeu_ps(&C.data[(i0) * (C.strides[0])], C_reg[i0][0]);
  _mm512_storeu_ps(&C.data[(i0) * (C.strides[0]) + 16], C_reg[i0][1]);
  _mm512_storeu_ps(&C.data[(i0) * (C.strides[0]) + 32], C_reg[i0][2]);
  _mm512_storeu_ps(&C.data[(i0) * (C.strides[0]) + 48], C_reg[i0][3]);
}
}

// basic_kernel_3x4(
//     K : size,
//     A : [f32][3, K] @DRAM,
//     B : [f32][K, 64] @DRAM,
//     C : [f32][3, 64] @DRAM
// )
static void basic_kernel_3x4( void *ctxt, int_fast32_t K, struct exo_win_2f32c A, struct exo_win_2f32c B, struct exo_win_2f32 C ) {
EXO_ASSUME(K >= 1);
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
__m512 C_reg[3][4];
for (int_fast32_t i0 = 0; i0 < 3; i0++) {
  C_reg[i0][0] = _mm512_loadu_ps(&C.data[(i0) * (C.strides[0])]);
  C_reg[i0][1] = _mm512_loadu_ps(&C.data[(i0) * (C.strides[0]) + 16]);
  C_reg[i0][2] = _mm512_loadu_ps(&C.data[(i0) * (C.strides[0]) + 32]);
  C_reg[i0][3] = _mm512_loadu_ps(&C.data[(i0) * (C.strides[0]) + 48]);
}
for (int_fast32_t k = 0; k < K; k++) {
  for (int_fast32_t i = 0; i < 3; i++) {
    __m512 var0;
    var0 = _mm512_set1_ps(A.data[(i) * (A.strides[0]) + k]);
    __m512 var1;
    var1 = _mm512_loadu_ps(&B.data[(k) * (B.strides[0])]);
    C_reg[i][0] = _mm512_fmadd_ps(var0, var1, C_reg[i][0]);
    __m512 var1_1;
    var1_1 = _mm512_loadu_ps(&B.data[(k) * (B.strides[0]) + 16]);
    C_reg[i][1] = _mm512_fmadd_ps(var0, var1_1, C_reg[i][1]);
    __m512 var1_2;
    var1_2 = _mm512_loadu_ps(&B.data[(k) * (B.strides[0]) + 32]);
    C_reg[i][2] = _mm512_fmadd_ps(var0, var1_2, C_reg[i][2]);
    __m512 var1_3;
    var1_3 = _mm512_loadu_ps(&B.data[(k) * (B.strides[0]) + 48]);
    C_reg[i][3] = _mm512_fmadd_ps(var0, var1_3, C_reg[i][3]);
  }
}
for (int_fast32_t i0 = 0; i0 < 3; i0++) {
  _mm512_storeu_ps(&C.data[(i0) * (C.strides[0])], C_reg[i0][0]);
  _mm512_storeu_ps(&C.data[(i0) * (C.strides[0]) + 16], C_reg[i0][1]);
  _mm512_storeu_ps(&C.data[(i0) * (C.strides[0]) + 32], C_reg[i0][2]);
  _mm512_storeu_ps(&C.data[(i0) * (C.strides[0]) + 48], C_reg[i0][3]);
}
}

// basic_kernel_4x4(
//     K : size,
//     A : [f32][4, K] @DRAM,
//     B : [f32][K, 64] @DRAM,
//     C : [f32][4, 64] @DRAM
// )
static void basic_kernel_4x4( void *ctxt, int_fast32_t K, struct exo_win_2f32c A, struct exo_win_2f32c B, struct exo_win_2f32 C ) {
EXO_ASSUME(K >= 1);
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
__m512 C_reg[4][4];
for (int_fast32_t i0 = 0; i0 < 4; i0++) {
  C_reg[i0][0] = _mm512_loadu_ps(&C.data[(i0) * (C.strides[0])]);
  C_reg[i0][1] = _mm512_loadu_ps(&C.data[(i0) * (C.strides[0]) + 16]);
  C_reg[i0][2] = _mm512_loadu_ps(&C.data[(i0) * (C.strides[0]) + 32]);
  C_reg[i0][3] = _mm512_loadu_ps(&C.data[(i0) * (C.strides[0]) + 48]);
}
for (int_fast32_t k = 0; k < K; k++) {
  for (int_fast32_t i = 0; i < 4; i++) {
    __m512 var0;
    var0 = _mm512_set1_ps(A.data[(i) * (A.strides[0]) + k]);
    __m512 var1;
    var1 = _mm512_loadu_ps(&B.data[(k) * (B.strides[0])]);
    C_reg[i][0] = _mm512_fmadd_ps(var0, var1, C_reg[i][0]);
    __m512 var1_1;
    var1_1 = _mm512_loadu_ps(&B.data[(k) * (B.strides[0]) + 16]);
    C_reg[i][1] = _mm512_fmadd_ps(var0, var1_1, C_reg[i][1]);
    __m512 var1_2;
    var1_2 = _mm512_loadu_ps(&B.data[(k) * (B.strides[0]) + 32]);
    C_reg[i][2] = _mm512_fmadd_ps(var0, var1_2, C_reg[i][2]);
    __m512 var1_3;
    var1_3 = _mm512_loadu_ps(&B.data[(k) * (B.strides[0]) + 48]);
    C_reg[i][3] = _mm512_fmadd_ps(var0, var1_3, C_reg[i][3]);
  }
}
for (int_fast32_t i0 = 0; i0 < 4; i0++) {
  _mm512_storeu_ps(&C.data[(i0) * (C.strides[0])], C_reg[i0][0]);
  _mm512_storeu_ps(&C.data[(i0) * (C.strides[0]) + 16], C_reg[i0][1]);
  _mm512_storeu_ps(&C.data[(i0) * (C.strides[0]) + 32], C_reg[i0][2]);
  _mm512_storeu_ps(&C.data[(i0) * (C.strides[0]) + 48], C_reg[i0][3]);
}
}

// basic_kernel_5x4(
//     K : size,
//     A : [f32][5, K] @DRAM,
//     B : [f32][K, 64] @DRAM,
//     C : [f32][5, 64] @DRAM
// )
static void basic_kernel_5x4( void *ctxt, int_fast32_t K, struct exo_win_2f32c A, struct exo_win_2f32c B, struct exo_win_2f32 C ) {
EXO_ASSUME(K >= 1);
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
__m512 C_reg[5][4];
for (int_fast32_t i0 = 0; i0 < 5; i0++) {
  C_reg[i0][0] = _mm512_loadu_ps(&C.data[(i0) * (C.strides[0])]);
  C_reg[i0][1] = _mm512_loadu_ps(&C.data[(i0) * (C.strides[0]) + 16]);
  C_reg[i0][2] = _mm512_loadu_ps(&C.data[(i0) * (C.strides[0]) + 32]);
  C_reg[i0][3] = _mm512_loadu_ps(&C.data[(i0) * (C.strides[0]) + 48]);
}
for (int_fast32_t k = 0; k < K; k++) {
  for (int_fast32_t i = 0; i < 5; i++) {
    __m512 var0;
    var0 = _mm512_set1_ps(A.data[(i) * (A.strides[0]) + k]);
    __m512 var1;
    var1 = _mm512_loadu_ps(&B.data[(k) * (B.strides[0])]);
    C_reg[i][0] = _mm512_fmadd_ps(var0, var1, C_reg[i][0]);
    __m512 var1_1;
    var1_1 = _mm512_loadu_ps(&B.data[(k) * (B.strides[0]) + 16]);
    C_reg[i][1] = _mm512_fmadd_ps(var0, var1_1, C_reg[i][1]);
    __m512 var1_2;
    var1_2 = _mm512_loadu_ps(&B.data[(k) * (B.strides[0]) + 32]);
    C_reg[i][2] = _mm512_fmadd_ps(var0, var1_2, C_reg[i][2]);
    __m512 var1_3;
    var1_3 = _mm512_loadu_ps(&B.data[(k) * (B.strides[0]) + 48]);
    C_reg[i][3] = _mm512_fmadd_ps(var0, var1_3, C_reg[i][3]);
  }
}
for (int_fast32_t i0 = 0; i0 < 5; i0++) {
  _mm512_storeu_ps(&C.data[(i0) * (C.strides[0])], C_reg[i0][0]);
  _mm512_storeu_ps(&C.data[(i0) * (C.strides[0]) + 16], C_reg[i0][1]);
  _mm512_storeu_ps(&C.data[(i0) * (C.strides[0]) + 32], C_reg[i0][2]);
  _mm512_storeu_ps(&C.data[(i0) * (C.strides[0]) + 48], C_reg[i0][3]);
}
}

// basic_kernel_6x4(
//     K : size,
//     A : [f32][6, K] @DRAM,
//     B : [f32][K, 64] @DRAM,
//     C : [f32][6, 64] @DRAM
// )
static void basic_kernel_6x4( void *ctxt, int_fast32_t K, struct exo_win_2f32c A, struct exo_win_2f32c B, struct exo_win_2f32 C ) {
EXO_ASSUME(K >= 1);
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
__m512 C_reg[6][4];
for (int_fast32_t i0 = 0; i0 < 6; i0++) {
  C_reg[i0][0] = _mm512_loadu_ps(&C.data[(i0) * (C.strides[0])]);
  C_reg[i0][1] = _mm512_loadu_ps(&C.data[(i0) * (C.strides[0]) + 16]);
  C_reg[i0][2] = _mm512_loadu_ps(&C.data[(i0) * (C.strides[0]) + 32]);
  C_reg[i0][3] = _mm512_loadu_ps(&C.data[(i0) * (C.strides[0]) + 48]);
}
for (int_fast32_t k = 0; k < K; k++) {
  for (int_fast32_t i = 0; i < 6; i++) {
    __m512 var0;
    var0 = _mm512_set1_ps(A.data[(i) * (A.strides[0]) + k]);
    __m512 var1;
    var1 = _mm512_loadu_ps(&B.data[(k) * (B.strides[0])]);
    C_reg[i][0] = _mm512_fmadd_ps(var0, var1, C_reg[i][0]);
    __m512 var1_1;
    var1_1 = _mm512_loadu_ps(&B.data[(k) * (B.strides[0]) + 16]);
    C_reg[i][1] = _mm512_fmadd_ps(var0, var1_1, C_reg[i][1]);
    __m512 var1_2;
    var1_2 = _mm512_loadu_ps(&B.data[(k) * (B.strides[0]) + 32]);
    C_reg[i][2] = _mm512_fmadd_ps(var0, var1_2, C_reg[i][2]);
    __m512 var1_3;
    var1_3 = _mm512_loadu_ps(&B.data[(k) * (B.strides[0]) + 48]);
    C_reg[i][3] = _mm512_fmadd_ps(var0, var1_3, C_reg[i][3]);
  }
}
for (int_fast32_t i0 = 0; i0 < 6; i0++) {
  _mm512_storeu_ps(&C.data[(i0) * (C.strides[0])], C_reg[i0][0]);
  _mm512_storeu_ps(&C.data[(i0) * (C.strides[0]) + 16], C_reg[i0][1]);
  _mm512_storeu_ps(&C.data[(i0) * (C.strides[0]) + 32], C_reg[i0][2]);
  _mm512_storeu_ps(&C.data[(i0) * (C.strides[0]) + 48], C_reg[i0][3]);
}
}

// bottom_panel_kernel_scheduled(
//     M : size,
//     K : size,
//     A : [f32][M, K] @DRAM,
//     B : [f32][K, 64] @DRAM,
//     C : [f32][M, 64] @DRAM
// )
static void bottom_panel_kernel_scheduled( void *ctxt, int_fast32_t M, int_fast32_t K, struct exo_win_2f32c A, struct exo_win_2f32c B, struct exo_win_2f32 C ) {
EXO_ASSUME(M >= 1);
EXO_ASSUME(K >= 1);
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
EXO_ASSUME(M < 6);
if (M == 1) {
  basic_kernel_1x4(ctxt,K,(struct exo_win_2f32c){ &A.data[0], { A.strides[0], 1 } },(struct exo_win_2f32c){ &B.data[0], { B.strides[0], 1 } },(struct exo_win_2f32){ &C.data[0], { C.strides[0], 1 } });
} else {
  if (M == 2) {
    basic_kernel_2x4(ctxt,K,(struct exo_win_2f32c){ &A.data[0], { A.strides[0], 1 } },(struct exo_win_2f32c){ &B.data[0], { B.strides[0], 1 } },(struct exo_win_2f32){ &C.data[0], { C.strides[0], 1 } });
  } else {
    if (M == 3) {
      basic_kernel_3x4(ctxt,K,(struct exo_win_2f32c){ &A.data[0], { A.strides[0], 1 } },(struct exo_win_2f32c){ &B.data[0], { B.strides[0], 1 } },(struct exo_win_2f32){ &C.data[0], { C.strides[0], 1 } });
    } else {
      if (M == 4) {
        basic_kernel_4x4(ctxt,K,(struct exo_win_2f32c){ &A.data[0], { A.strides[0], 1 } },(struct exo_win_2f32c){ &B.data[0], { B.strides[0], 1 } },(struct exo_win_2f32){ &C.data[0], { C.strides[0], 1 } });
      } else {
        basic_kernel_5x4(ctxt,K,(struct exo_win_2f32c){ &A.data[0], { A.strides[0], 1 } },(struct exo_win_2f32c){ &B.data[0], { B.strides[0], 1 } },(struct exo_win_2f32){ &C.data[0], { C.strides[0], 1 } });
      }
    }
  }
}
}


/* relying on the following instruction..."
mm512_fmadd_ps(A,B,C)
{C_data} = _mm512_fmadd_ps({A_data}, {B_data}, {C_data});
*/

/* relying on the following instruction..."
mm512_loadu_ps(dst,src)
{dst_data} = _mm512_loadu_ps(&{src_data});
*/

/* relying on the following instruction..."
mm512_mask_fmadd_ps(N,A,B,C)
{C_data} = _mm512_mask_fmadd_ps({A_data}, ((1 << {N}) - 1), {B_data}, {C_data});
*/

/* relying on the following instruction..."
mm512_mask_set1_ps(N,dst,src)
{dst_data} = _mm512_set1_ps({src_data});
*/

/* relying on the following instruction..."
mm512_mask_storeu_ps(N,dst,src)
_mm512_mask_storeu_ps(&{dst_data}, ((1 << {N}) - 1), {src_data});
*/

/* relying on the following instruction..."
mm512_maskz_loadu_ps(N,dst,src)
{dst_data} = _mm512_maskz_loadu_ps(((1 << {N}) - 1), &{src_data});
*/

/* relying on the following instruction..."
mm512_set1_ps(dst,src)
{dst_data} = _mm512_set1_ps({src_data});
*/

/* relying on the following instruction..."
mm512_storeu_ps(dst,src)
_mm512_storeu_ps(&{dst_data}, {src_data});
*/
// right_panel_kernel0(
//     N : size,
//     K : size,
//     A : [f32][6, K] @DRAM,
//     B : [f32][K, N] @DRAM,
//     C : [f32][6, N] @DRAM
// )
static void right_panel_kernel0( void *ctxt, int_fast32_t N, int_fast32_t K, struct exo_win_2f32c A, struct exo_win_2f32c B, struct exo_win_2f32 C ) {
EXO_ASSUME(N >= 1);
EXO_ASSUME(K >= 1);
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
EXO_ASSUME(N < 64);
EXO_ASSUME(((15 + N) / (16)) == 1);
__m512 C_reg[6][1];
for (int_fast32_t i0 = 0; i0 < 6; i0++) {
  C_reg[i0][0] = _mm512_maskz_loadu_ps(((1 << (N)) - 1), &C.data[(i0) * (C.strides[0])]);
}
for (int_fast32_t k = 0; k < K; k++) {
  for (int_fast32_t i = 0; i < 6; i++) {
    __m512 var0;
    __m512 var1;
    var0 = _mm512_set1_ps(A.data[(i) * (A.strides[0]) + k]);
    var1 = _mm512_maskz_loadu_ps(((1 << (N)) - 1), &B.data[(k) * (B.strides[0])]);
    C_reg[i][0] = _mm512_mask_fmadd_ps(var0, ((1 << (N)) - 1), var1, C_reg[i][0]);
  }
}
for (int_fast32_t i0 = 0; i0 < 6; i0++) {
  _mm512_mask_storeu_ps(&C.data[(i0) * (C.strides[0])], ((1 << (N)) - 1), C_reg[i0][0]);
}
}

// right_panel_kernel1(
//     N : size,
//     K : size,
//     A : [f32][6, K] @DRAM,
//     B : [f32][K, N] @DRAM,
//     C : [f32][6, N] @DRAM
// )
static void right_panel_kernel1( void *ctxt, int_fast32_t N, int_fast32_t K, struct exo_win_2f32c A, struct exo_win_2f32c B, struct exo_win_2f32 C ) {
EXO_ASSUME(N >= 1);
EXO_ASSUME(K >= 1);
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
EXO_ASSUME(N < 64);
EXO_ASSUME(((15 + N) / (16)) == 2);
__m512 C_reg[6][2];
for (int_fast32_t i0 = 0; i0 < 6; i0++) {
  C_reg[i0][0] = _mm512_loadu_ps(&C.data[(i0) * (C.strides[0])]);
  C_reg[i0][1] = _mm512_maskz_loadu_ps(((1 << (-16 + N)) - 1), &C.data[(i0) * (C.strides[0]) + 16]);
}
for (int_fast32_t k = 0; k < K; k++) {
  for (int_fast32_t i = 0; i < 6; i++) {
    __m512 var0;
    __m512 var1;
    var0 = _mm512_set1_ps(A.data[(i) * (A.strides[0]) + k]);
    var1 = _mm512_loadu_ps(&B.data[(k) * (B.strides[0])]);
    C_reg[i][0] = _mm512_fmadd_ps(var0, var1, C_reg[i][0]);
    __m512 var0_1;
    __m512 var1_1;
    var0_1 = _mm512_set1_ps(A.data[(i) * (A.strides[0]) + k]);
    var1_1 = _mm512_maskz_loadu_ps(((1 << (-16 + N)) - 1), &B.data[(k) * (B.strides[0]) + 16]);
    C_reg[i][1] = _mm512_mask_fmadd_ps(var0_1, ((1 << (-16 + N)) - 1), var1_1, C_reg[i][1]);
  }
}
for (int_fast32_t i0 = 0; i0 < 6; i0++) {
  _mm512_storeu_ps(&C.data[(i0) * (C.strides[0])], C_reg[i0][0]);
  _mm512_mask_storeu_ps(&C.data[(i0) * (C.strides[0]) + 16], ((1 << (-16 + N)) - 1), C_reg[i0][1]);
}
}

// right_panel_kernel2(
//     N : size,
//     K : size,
//     A : [f32][6, K] @DRAM,
//     B : [f32][K, N] @DRAM,
//     C : [f32][6, N] @DRAM
// )
static void right_panel_kernel2( void *ctxt, int_fast32_t N, int_fast32_t K, struct exo_win_2f32c A, struct exo_win_2f32c B, struct exo_win_2f32 C ) {
EXO_ASSUME(N >= 1);
EXO_ASSUME(K >= 1);
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
EXO_ASSUME(N < 64);
EXO_ASSUME(((15 + N) / (16)) == 3);
__m512 C_reg[6][3];
for (int_fast32_t i0 = 0; i0 < 6; i0++) {
  C_reg[i0][0] = _mm512_loadu_ps(&C.data[(i0) * (C.strides[0])]);
  C_reg[i0][1] = _mm512_loadu_ps(&C.data[(i0) * (C.strides[0]) + 16]);
  C_reg[i0][2] = _mm512_maskz_loadu_ps(((1 << (-32 + N)) - 1), &C.data[(i0) * (C.strides[0]) + 32]);
}
for (int_fast32_t k = 0; k < K; k++) {
  for (int_fast32_t i = 0; i < 6; i++) {
    __m512 var0;
    __m512 var1;
    var0 = _mm512_set1_ps(A.data[(i) * (A.strides[0]) + k]);
    var1 = _mm512_loadu_ps(&B.data[(k) * (B.strides[0])]);
    C_reg[i][0] = _mm512_fmadd_ps(var0, var1, C_reg[i][0]);
    __m512 var0_1;
    __m512 var1_1;
    var0_1 = _mm512_set1_ps(A.data[(i) * (A.strides[0]) + k]);
    var1_1 = _mm512_loadu_ps(&B.data[(k) * (B.strides[0]) + 16]);
    C_reg[i][1] = _mm512_fmadd_ps(var0_1, var1_1, C_reg[i][1]);
    __m512 var0_2;
    __m512 var1_2;
    var0_2 = _mm512_set1_ps(A.data[(i) * (A.strides[0]) + k]);
    var1_2 = _mm512_maskz_loadu_ps(((1 << (-32 + N)) - 1), &B.data[(k) * (B.strides[0]) + 32]);
    C_reg[i][2] = _mm512_mask_fmadd_ps(var0_2, ((1 << (-32 + N)) - 1), var1_2, C_reg[i][2]);
  }
}
for (int_fast32_t i0 = 0; i0 < 6; i0++) {
  _mm512_storeu_ps(&C.data[(i0) * (C.strides[0])], C_reg[i0][0]);
  _mm512_storeu_ps(&C.data[(i0) * (C.strides[0]) + 16], C_reg[i0][1]);
  _mm512_mask_storeu_ps(&C.data[(i0) * (C.strides[0]) + 32], ((1 << (-32 + N)) - 1), C_reg[i0][2]);
}
}

// right_panel_kernel3(
//     N : size,
//     K : size,
//     A : [f32][6, K] @DRAM,
//     B : [f32][K, N] @DRAM,
//     C : [f32][6, N] @DRAM
// )
static void right_panel_kernel3( void *ctxt, int_fast32_t N, int_fast32_t K, struct exo_win_2f32c A, struct exo_win_2f32c B, struct exo_win_2f32 C ) {
EXO_ASSUME(N >= 1);
EXO_ASSUME(K >= 1);
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
EXO_ASSUME(N < 64);
EXO_ASSUME(((15 + N) / (16)) == 4);
__m512 C_reg[6][4];
for (int_fast32_t i0 = 0; i0 < 6; i0++) {
  C_reg[i0][0] = _mm512_loadu_ps(&C.data[(i0) * (C.strides[0])]);
  C_reg[i0][1] = _mm512_loadu_ps(&C.data[(i0) * (C.strides[0]) + 16]);
  C_reg[i0][2] = _mm512_loadu_ps(&C.data[(i0) * (C.strides[0]) + 32]);
  C_reg[i0][3] = _mm512_maskz_loadu_ps(((1 << (-48 + N)) - 1), &C.data[(i0) * (C.strides[0]) + 48]);
}
for (int_fast32_t k = 0; k < K; k++) {
  for (int_fast32_t i = 0; i < 6; i++) {
    __m512 var0;
    __m512 var1;
    var0 = _mm512_set1_ps(A.data[(i) * (A.strides[0]) + k]);
    var1 = _mm512_loadu_ps(&B.data[(k) * (B.strides[0])]);
    C_reg[i][0] = _mm512_fmadd_ps(var0, var1, C_reg[i][0]);
    __m512 var0_1;
    __m512 var1_1;
    var0_1 = _mm512_set1_ps(A.data[(i) * (A.strides[0]) + k]);
    var1_1 = _mm512_loadu_ps(&B.data[(k) * (B.strides[0]) + 16]);
    C_reg[i][1] = _mm512_fmadd_ps(var0_1, var1_1, C_reg[i][1]);
    __m512 var0_2;
    __m512 var1_2;
    var0_2 = _mm512_set1_ps(A.data[(i) * (A.strides[0]) + k]);
    var1_2 = _mm512_loadu_ps(&B.data[(k) * (B.strides[0]) + 32]);
    C_reg[i][2] = _mm512_fmadd_ps(var0_2, var1_2, C_reg[i][2]);
    __m512 var0_3;
    __m512 var1_3;
    var0_3 = _mm512_set1_ps(A.data[(i) * (A.strides[0]) + k]);
    var1_3 = _mm512_maskz_loadu_ps(((1 << (-48 + N)) - 1), &B.data[(k) * (B.strides[0]) + 48]);
    C_reg[i][3] = _mm512_mask_fmadd_ps(var0_3, ((1 << (-48 + N)) - 1), var1_3, C_reg[i][3]);
  }
}
for (int_fast32_t i0 = 0; i0 < 6; i0++) {
  _mm512_storeu_ps(&C.data[(i0) * (C.strides[0])], C_reg[i0][0]);
  _mm512_storeu_ps(&C.data[(i0) * (C.strides[0]) + 16], C_reg[i0][1]);
  _mm512_storeu_ps(&C.data[(i0) * (C.strides[0]) + 32], C_reg[i0][2]);
  _mm512_mask_storeu_ps(&C.data[(i0) * (C.strides[0]) + 48], ((1 << (-48 + N)) - 1), C_reg[i0][3]);
}
}

// right_panel_kernel_scheduled(
//     N : size,
//     K : size,
//     A : [f32][6, K] @DRAM,
//     B : [f32][K, N] @DRAM,
//     C : [f32][6, N] @DRAM
// )
static void right_panel_kernel_scheduled( void *ctxt, int_fast32_t N, int_fast32_t K, struct exo_win_2f32c A, struct exo_win_2f32c B, struct exo_win_2f32 C ) {
EXO_ASSUME(N >= 1);
EXO_ASSUME(K >= 1);
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
EXO_ASSUME(N < 64);
if (((N + 15) / (16)) == 1) {
  right_panel_kernel0(ctxt,N + 0,K + 0,(struct exo_win_2f32c){ &A.data[0], { A.strides[0], 1 } },(struct exo_win_2f32c){ &B.data[0], { B.strides[0], 1 } },(struct exo_win_2f32){ &C.data[0], { C.strides[0], 1 } });
} else {
  if (((N + 15) / (16)) == 2) {
    right_panel_kernel1(ctxt,N + 0,K + 0,(struct exo_win_2f32c){ &A.data[0], { A.strides[0], 1 } },(struct exo_win_2f32c){ &B.data[0], { B.strides[0], 1 } },(struct exo_win_2f32){ &C.data[0], { C.strides[0], 1 } });
  } else {
    if (((N + 15) / (16)) == 3) {
      right_panel_kernel2(ctxt,N + 0,K + 0,(struct exo_win_2f32c){ &A.data[0], { A.strides[0], 1 } },(struct exo_win_2f32c){ &B.data[0], { B.strides[0], 1 } },(struct exo_win_2f32){ &C.data[0], { C.strides[0], 1 } });
    } else {
      right_panel_kernel3(ctxt,N + 0,K + 0,(struct exo_win_2f32c){ &A.data[0], { A.strides[0], 1 } },(struct exo_win_2f32c){ &B.data[0], { B.strides[0], 1 } },(struct exo_win_2f32){ &C.data[0], { C.strides[0], 1 } });
    }
  }
}
}

// sgemm_above_kernel(
//     M : size,
//     N : size,
//     K : size,
//     A : [f32][M, K] @DRAM,
//     B : [f32][K, N] @DRAM,
//     C : [f32][M, N] @DRAM
// )
static void sgemm_above_kernel( void *ctxt, int_fast32_t M, int_fast32_t N, int_fast32_t K, struct exo_win_2f32c A, struct exo_win_2f32c B, struct exo_win_2f32 C ) {
EXO_ASSUME(M >= 1);
EXO_ASSUME(N >= 1);
EXO_ASSUME(K >= 1);
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
for (int_fast32_t io = 0; io < ((M) / (6)); io++) {
  for (int_fast32_t jo = 0; jo < ((N) / (64)); jo++) {
    basic_kernel_6x4(ctxt,K,(struct exo_win_2f32c){ &A.data[(6 * io) * (A.strides[0])], { A.strides[0], 1 } },(struct exo_win_2f32c){ &B.data[64 * jo], { B.strides[0], 1 } },(struct exo_win_2f32){ &C.data[(6 * io) * (C.strides[0]) + 64 * jo], { C.strides[0], 1 } });
  }
}
for (int_fast32_t io = 0; io < ((M) / (6)); io++) {
  if (N % 64 > 0) {
    right_panel_kernel_scheduled(ctxt,N % 64,K,(struct exo_win_2f32c){ &A.data[(6 * io) * (A.strides[0])], { A.strides[0], 1 } },(struct exo_win_2f32c){ &B.data[64 * (N / 64)], { B.strides[0], 1 } },(struct exo_win_2f32){ &C.data[(6 * io) * (C.strides[0]) + 64 * (N / 64)], { C.strides[0], 1 } });
  }
}
if (M % 6 > 0) {
  for (int_fast32_t jo = 0; jo < ((N) / (64)); jo++) {
    bottom_panel_kernel_scheduled(ctxt,M % 6,K,(struct exo_win_2f32c){ &A.data[(6 * (M / 6)) * (A.strides[0])], { A.strides[0], 1 } },(struct exo_win_2f32c){ &B.data[64 * jo], { B.strides[0], 1 } },(struct exo_win_2f32){ &C.data[(6 * (M / 6)) * (C.strides[0]) + 64 * jo], { C.strides[0], 1 } });
  }
  if (N % 64 > 0) {
    for (int_fast32_t k = 0; k < K; k++) {
      for (int_fast32_t ii = 0; ii < M % 6; ii++) {
        for (int_fast32_t ji = 0; ji < N % 64; ji++) {
          C.data[(ii + (M / 6) * 6) * C.strides[0] + ji + (N / 64) * 64] += A.data[(ii + (M / 6) * 6) * A.strides[0] + k] * B.data[k * B.strides[0] + ji + (N / 64) * 64];
        }
      }
    }
  }
}
}

// sgemm_exo(
//     M : size,
//     N : size,
//     K : size,
//     A : f32[M, K] @DRAM,
//     B : f32[K, N] @DRAM,
//     C : f32[M, N] @DRAM
// )
void sgemm_exo( void *ctxt, int_fast32_t M, int_fast32_t N, int_fast32_t K, const float* A, const float* B, float* C ) {
EXO_ASSUME(M >= 1);
EXO_ASSUME(N >= 1);
EXO_ASSUME(K >= 1);
// assert stride(A, 1) == 1
// assert stride(B, 1) == 1
// assert stride(C, 1) == 1
for (int_fast32_t ko = 0; ko < ((K) / (512)); ko++) {
  for (int_fast32_t io = 0; io < ((M) / (264)); io++) {
    static float A_cache[264 * 512];
    for (int_fast32_t i0 = 0; i0 < 264; i0++) {
      for (int_fast32_t i1 = 0; i1 < 512; i1++) {
        A_cache[i0 * 512 + i1] = A[(i0 + 264 * io) * K + i1 + 512 * ko];
      }
    }
    for (int_fast32_t jo = 0; jo < ((N) / (64)); jo++) {
      static float B_cache[512 * 64];
      for (int_fast32_t i0 = 0; i0 < 512; i0++) {
        for (int_fast32_t i1 = 0; i1 < 64; i1++) {
          B_cache[i0 * 64 + i1] = B[(i0 + 512 * ko) * N + i1 + 64 * jo];
        }
      }
      sgemm_above_kernel(ctxt,264,64,512,(struct exo_win_2f32c){ &A_cache[0], { 512, 1 } },(struct exo_win_2f32c){ &B_cache[0], { 64, 1 } },(struct exo_win_2f32){ &C[(264 * io) * N + 64 * jo], { N, 1 } });
    }
  }
}
for (int_fast32_t ko = 0; ko < ((K) / (512)); ko++) {
  for (int_fast32_t io = 0; io < ((M) / (264)); io++) {
    if (N % 64 > 0) {
      static float B_cache[512 * 64];
      for (int_fast32_t i0 = 0; i0 < 512; i0++) {
        for (int_fast32_t i1 = 0; i1 < N - ((N) / (64)) * 64; i1++) {
          B_cache[i0 * 64 + i1] = B[(i0 + 512 * ko) * N + i1 + (N / 64) * 64];
        }
      }
      sgemm_above_kernel(ctxt,264,N % 64,512,(struct exo_win_2f32c){ &A[(264 * io) * K + 512 * ko], { K, 1 } },(struct exo_win_2f32c){ &B_cache[0], { 64, 1 } },(struct exo_win_2f32){ &C[(264 * io) * N + 64 * (N / 64)], { N, 1 } });
    }
  }
  if (M % 264 > 0) {
    for (int_fast32_t jo = 0; jo < ((N) / (64)); jo++) {
      static float B_cache[512 * 64];
      for (int_fast32_t i0 = 0; i0 < 512; i0++) {
        for (int_fast32_t i1 = 0; i1 < 64; i1++) {
          B_cache[i0 * 64 + i1] = B[(i0 + 512 * ko) * N + i1 + 64 * jo];
        }
      }
      sgemm_above_kernel(ctxt,M % 264,64,512,(struct exo_win_2f32c){ &A[(264 * (M / 264)) * K + 512 * ko], { K, 1 } },(struct exo_win_2f32c){ &B_cache[0], { 64, 1 } },(struct exo_win_2f32){ &C[(264 * (M / 264)) * N + 64 * jo], { N, 1 } });
    }
    if (N % 64 > 0) {
      static float B_cache[512 * 64];
      for (int_fast32_t i0 = 0; i0 < 512; i0++) {
        for (int_fast32_t i1 = 0; i1 < N - ((N) / (64)) * 64; i1++) {
          B_cache[i0 * 64 + i1] = B[(i0 + 512 * ko) * N + i1 + (N / 64) * 64];
        }
      }
      sgemm_above_kernel(ctxt,M % 264,N % 64,512,(struct exo_win_2f32c){ &A[(264 * (M / 264)) * K + 512 * ko], { K, 1 } },(struct exo_win_2f32c){ &B_cache[0], { 64, 1 } },(struct exo_win_2f32){ &C[(264 * (M / 264)) * N + 64 * (N / 64)], { N, 1 } });
    }
  }
}
if (K % 512 > 0) {
  for (int_fast32_t io = 0; io < ((M) / (264)); io++) {
    for (int_fast32_t jo = 0; jo < ((N) / (64)); jo++) {
      static float B_cache[512 * 64];
      for (int_fast32_t i0 = 0; i0 < K - ((K) / (512)) * 512; i0++) {
        for (int_fast32_t i1 = 0; i1 < 64; i1++) {
          B_cache[i0 * 64 + i1] = B[(i0 + (K / 512) * 512) * N + i1 + 64 * jo];
        }
      }
      sgemm_above_kernel(ctxt,264,64,K % 512,(struct exo_win_2f32c){ &A[(264 * io) * K + 512 * (K / 512)], { K, 1 } },(struct exo_win_2f32c){ &B_cache[0], { 64, 1 } },(struct exo_win_2f32){ &C[(264 * io) * N + 64 * jo], { N, 1 } });
    }
    if (N % 64 > 0) {
      static float B_cache[512 * 64];
      for (int_fast32_t i0 = 0; i0 < K - ((K) / (512)) * 512; i0++) {
        for (int_fast32_t i1 = 0; i1 < N - ((N) / (64)) * 64; i1++) {
          B_cache[i0 * 64 + i1] = B[(i0 + (K / 512) * 512) * N + i1 + (N / 64) * 64];
        }
      }
      sgemm_above_kernel(ctxt,264,N % 64,K % 512,(struct exo_win_2f32c){ &A[(264 * io) * K + 512 * (K / 512)], { K, 1 } },(struct exo_win_2f32c){ &B_cache[0], { 64, 1 } },(struct exo_win_2f32){ &C[(264 * io) * N + 64 * (N / 64)], { N, 1 } });
    }
  }
  if (M % 264 > 0) {
    for (int_fast32_t jo = 0; jo < ((N) / (64)); jo++) {
      static float B_cache[512 * 64];
      for (int_fast32_t i0 = 0; i0 < K - ((K) / (512)) * 512; i0++) {
        for (int_fast32_t i1 = 0; i1 < 64; i1++) {
          B_cache[i0 * 64 + i1] = B[(i0 + (K / 512) * 512) * N + i1 + 64 * jo];
        }
      }
      sgemm_above_kernel(ctxt,M % 264,64,K % 512,(struct exo_win_2f32c){ &A[(264 * (M / 264)) * K + 512 * (K / 512)], { K, 1 } },(struct exo_win_2f32c){ &B_cache[0], { 64, 1 } },(struct exo_win_2f32){ &C[(264 * (M / 264)) * N + 64 * jo], { N, 1 } });
    }
    if (N % 64 > 0) {
      static float B_cache[512 * 64];
      for (int_fast32_t i0 = 0; i0 < K - ((K) / (512)) * 512; i0++) {
        for (int_fast32_t i1 = 0; i1 < N - ((N) / (64)) * 64; i1++) {
          B_cache[i0 * 64 + i1] = B[(i0 + (K / 512) * 512) * N + i1 + (N / 64) * 64];
        }
      }
      sgemm_above_kernel(ctxt,M % 264,N % 64,K % 512,(struct exo_win_2f32c){ &A[(264 * (M / 264)) * K + 512 * (K / 512)], { K, 1 } },(struct exo_win_2f32c){ &B_cache[0], { 64, 1 } },(struct exo_win_2f32){ &C[(264 * (M / 264)) * N + 64 * (N / 64)], { N, 1 } });
    }
  }
}
}

